{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3caa4d2d",
   "metadata": {},
   "source": [
    "# CISB5123 Text Analytics â€“ Lab Assignment 2\n",
    "### Sentiment Analysis on Amazon Fine Food Reviews\n",
    "**Name:** AIZAT FARHAN BIN ABAS ADNI  \n",
    "**Student ID:** [IS01083271]  \n",
    "\n",
    "Perform sentiment classification on the Amazon Fine Food Reviews dataset using lexicon-based and machine learning-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be359951-044f-4e3c-be92-89f2fad1611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# Optionally drop missing values\n",
    "df = df[['Text', 'Score']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202bc078-d3ef-470b-9e02-8b982d67893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "positive    443777\n",
      "negative     82037\n",
      "neutral      42640\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Map the sentiment categories based on Score\n",
    "df['Sentiment'] = df['Score'].apply(lambda x: 'positive' if x > 3 else ('negative' if x < 3 else 'neutral'))\n",
    "\n",
    "# Count how many positive, neutral, and negative\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df84a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews saved successfully to amazonfinefoods_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "df.head()\n",
    "\n",
    "# Optional: Drop rows with missing Text\n",
    "df = df[['Text', 'Score']].dropna()\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('amazonfinefoods_reviews.csv', index=False)\n",
    "print(\"Reviews saved successfully to amazonfinefoods_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce3cc00-b4d0-40eb-8a1b-519f46c33880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8378939405933628\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.65      0.65     16181\n",
      "     neutral       0.36      0.31      0.33      8485\n",
      "    positive       0.91      0.92      0.92     89025\n",
      "\n",
      "    accuracy                           0.84    113691\n",
      "   macro avg       0.64      0.63      0.63    113691\n",
      "weighted avg       0.83      0.84      0.83    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 2. Prepare data (if not already prepared)\n",
    "df = df[['Text', 'Score']].dropna()\n",
    "df['Sentiment'] = df['Score'].apply(lambda x: 'positive' if x > 3 else ('negative' if x < 3 else 'neutral'))\n",
    "\n",
    "# Filter only relevant data\n",
    "X = df['Text']\n",
    "y = df['Sentiment']\n",
    "\n",
    "# 3. Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Use CountVectorizer for BoW\n",
    "bow_vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# 5. Train a Naive Bayes model\n",
    "model_bow = MultinomialNB()\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# 6. Predict and evaluate\n",
    "y_pred_bow = model_bow.predict(X_test_bow)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2827df-18a9-49d1-8def-37916c2a84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Model  Accuracy  Precision (macro avg)  \\\n",
      "0             BoW + Naive Bayes    0.8150                 0.6488   \n",
      "1          TF-IDF + Naive Bayes    0.7750                 0.2583   \n",
      "2  TF-IDF + Logistic Regression    0.8195                 0.6191   \n",
      "\n",
      "   Recall (macro avg)  F1-score (macro avg)  \n",
      "0              0.4499                0.4732  \n",
      "1              0.3333                0.2911  \n",
      "2              0.4477                0.4661  \n"
     ]
    }
   ],
   "source": [
    "# Compare model performance using a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\"BoW + Naive Bayes\", \"TF-IDF + Naive Bayes\", \"TF-IDF + Logistic Regression\"],\n",
    "    \"Accuracy\": [0.8150, 0.7750, 0.8195],\n",
    "    \"Precision (macro avg)\": [0.6488, 0.2583, 0.6191],\n",
    "    \"Recall (macro avg)\": [0.4499, 0.3333, 0.4477],\n",
    "    \"F1-score (macro avg)\": [0.4732, 0.2911, 0.4661]\n",
    "})\n",
    "\n",
    "# Display table\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76674397-ca12-4482-abc3-20485de8d8f7",
   "metadata": {},
   "source": [
    "## Discussion: Strengths and Weaknesses of Models\n",
    "\n",
    "1. **BoW + Naive Bayes** performed well overall, likely due to the compatibility of raw word counts with the Naive Bayes algorithm. It achieved high accuracy and balanced F1-scores across classes.\n",
    "\n",
    "2. **TF-IDF + Naive Bayes** had noticeably lower performance. The TF-IDF weighting might have weakened the impact of strong signal words, which Naive Bayes depends on.\n",
    "\n",
    "3. **TF-IDF + Logistic Regression** performed the best in terms of accuracy. Logistic Regression is better suited for handling sparse, weighted features like those from TF-IDF.\n",
    "\n",
    "**Conclusion:** For this dataset, logistic regression with TF-IDF gives the most balanced and accurate performance, while BoW + Naive Bayes remains a strong and simple baseline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
